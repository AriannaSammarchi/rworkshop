---
title: "Sample Analysis"
author: "Irfan Kanat"
date: "11/04/2015"
output: 
  pdf_document:
    fig_width: 4
    fig_height: 3
    fig_caption: false
---


## Statistical Models

In this section, I will try to provide an introduction to using two simple statistical models in R: regression and logistic regression.

### Regression

If your dependent variable is continuous you can simply use regression.

For this demonstration, I will use the same Motor Trends dataset I used in Visualization section. 

```{r}
data(mtcars) # Get the data
?mtcars # Help on dataset
```

We will use lm() function to fit regular regression.

```{r}
?lm
```

Below I declare a model where I use horse power, cylinders, and transmission type to estimate gas milage. Pay attention to model specification:

```
mpg ~ hp + cyl + am
```

Here the left hand side of the tilde is the dependent variable. and the right hand side has all the predictors we use separated by plus signs.

```{r}
# Fit 
reg_0 <- lm( mpg ~ hp + cyl + am, data = mtcars) 
summary(reg_0)
```

Look at the R-squared value to see how much variance is explained by the model, the more the better.

You can access estimated values as follows. I used a head function to limit the output.

```{r}
head(reg_0$fitted.values)
```

You can use the fitted model to predict new datasets. Here I am modifying Datsun710 to see how the gas milage may have been influenced if the car was automatic instead of manual transmission.

```{r}
newCar <- mtcars[3,] # 3rd observation is Datsun 710
newCar$am <- 0 # What if it was automatic?
predict(reg_0, newdata = newCar) # Estimate went down by 4 miles
```

One way to see how your model did is to plot residuals. Ideally the residuals should be close to 0 and randomly distributed. If you see a pattern, it indicates misspecification.

```{r}
library(ggplot2)
# Plot the fitted values against real values
qplot(data=mtcars, x = mpg, y = reg_0$residuals) +
  stat_smooth(method = "lm", col = "red")

# Are the residuals normally distributed? 
shapiro.test(reg_0$residuals) # yes
```

Comparing models. If you are using the same dataset, and just adding or removing variables to a model. You can compare models with a likelihood ratio test or an F test. Anova facilitates comparison of simple regression models.

```{r}
# Add variable wt
reg_1 <- lm( mpg ~ hp + cyl + am + wt, mtcars)

# Aikikae Information Criteria
# AIC lower the better
AIC(reg_0)
AIC(reg_1)

# Compare
anova(reg_0, reg_1) # models are significantly different
```

### Logistic Regression

Let us change gears and try to predict a binary variable. For this purpose we will use the logistic regression with a binomial link function. The model estimates the probability of Y=1.

Let us stick to the mtcars dataset and try to figure out if a car is automatic or manual based on predictors.

We will use glm function.

```{r}
?glm
```

Let us fit the model

```{r}
logit_2 <- glm(am ~ mpg + drat + cyl, data = mtcars, family='binomial')
summary(logit_2)
```

Visualize the results.

```{r}
ggplot(mtcars, aes(x = mpg, y = am)) + 
    stat_smooth(method="glm", family="binomial", se=FALSE)+
# Bonus: rename the y axis label
		ylab('Probability of Manual Transmission')
```

How about plotting results for number of cylinders? We will need to process the data a little bit.

```{r}
# Create a new dataset with varying number of cylinders and other variables fixed at mean levels.
mtcars2<-data.frame(mpg = rep(10:30, 3),drat = mean(mtcars$drat), disp = mean(mtcars$disp), cyl = rep(c(4,6,8),21))
# Predict probability of new data
mtcars2$prob<-predict(logit_2, newdata=mtcars2, type = "response")

# Plot the results
ggplot(mtcars2, aes(x=mpg, y=prob)) +
  geom_line(aes(colour = factor(cyl)), size = 1) 
```

Diagnostics with logistic regression.

```{r}
library(caret)

# Let us compare predicted values to real values
mtcars$prob <- predict(logit_2, type="response")
# Prevalence of Manual Transmission
mean(mtcars$am)

# Create predict variable
mtcars$pred <- 0
# If probability is greater than .6 (1-prevalence), set prediction to 1
mtcars[mtcars$prob>.6, 'pred'] <- 1

# Confusion Matrix
confusionMatrix(table(mtcars[,c("am", "pred")]))

## ROC CURVE
# Load the necessary library
library(pROC)
# Calculate the ROC curve using the predicted probability vs actual values
logit_2_roc <- roc(am~prob, mtcars) 
# Plot ROC curve
plot(logit_2_roc)
```


## Model Training - Discriminant Analysis with caret Package

In this document I will present a classification example using caret package. The example presented here follows closely [the caret package vignette](https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf). I only made a few changes to fit it into the workshop's time frame. So if you need any clarification, you can read up more in the vignette.

Caret package is a wrapper that brings together functionality from 27 packages. Caret supports [estimating over 150 models](http://topepo.github.io/caret/bytag.html) including bayesian, SVM, discriminant analysis, regressions, neural networks, and more.

```{r, eval=FALSE}
# Run below commands to get a list of related packages
available.packages()["caret","Depends"]
available.packages()["caret","Suggests"]
```

Caret package aims to be the go to package for your predictive analytics needs. Thus it not only covers model training, but also data manipulation, visualization, and parallelization capabilities.

Since so many packages involved, the installation takes a while.

```{r ,eval=FALSE}
install.packages("caret", dependencies = c("Depends", "Suggests"))
```

### Data

We will use the sonar dataset from mlbench package. It has 208 observations of 60 predictor variables and a binary class variable as dependent. We do not know what these 60 variables are.

```{r}
# Dataset comes with mlbench package
library(mlbench)
# Load dataset into the current workspace
data(Sonar)
```

### SPLIT THE DATA

Caret provides functionality to split the data into a training and a testing set while at the same time preserving the distribution of dependent variable.

createDataPartition function receives the dependent variable, proportion of training set in the whole of dataset as parameters.

```{r}
library(caret) # Load Library
set.seed(107)  # Set random number seed for reproducibility
# Create an index of observations to be included in Training
indexTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)

# Split the data using the index
Train <- Sonar[indexTrain,]
Test  <- Sonar[-indexTrain,]
```

### Training a Discriminant Model

We will be using a PLS DA model to train.

First step is to set resampling and validation strategy. Here we are using 3 resamplings of 10-fold cross validation. We also configure the trainer to produce predicted probabilities to be used in ROC calculation.

```{r}
## Declare Tuning Control Parameters
ctrl <- trainControl(method = "repeatedcv", # K-fold cross validation
			        repeats = 3, # Repeat resampling 3 times
                    classProbs = TRUE, # Calculate predicted prob for ROC
                    summaryFunction = twoClassSummary) # Set performance metrics for binary
```

Below we train the model with varying parameters. The tune length in this case specifies maximum the number of components to be extracted.

```{r}
plsFit <- train(Class ~ ., data = Train, method = "pls",
		tuneLength = 10,   # Number of component sets to be evaluated (more is better)
		trControl = ctrl, # Use control parameters from above
		metric = "ROC" ,  # Criteria ROC
		preProc = c("center", "scale")) # Center and scale the predictors
```

Let us review the models that were fit.

```{r}
plsFit
```

As you can see the area under the ROC curve increases up to 4 components and starts declining afterwards. By default the model with 4 components is used.

Plotting this fit would demonstrate the change in area under ROC for different components.

```{r}
# evaluate the performance of different number of components extracted
plot(plsFit)
```

### Validate with Test Data

Let us see how our fitted model performs with Test data.

```{r}
plsPredict <- predict(plsFit, newdata = Test) # Predict results
head(plsPredict) # View predictions
head(Test$Class) # View actual$
```

Get confusion matrix (predicted vs actual) for performance reports.

```{r}
confusionMatrix(data = plsPredict, Test$Class)
```


